# Comprehensive benchmark configuration with NVIDIA Nsight Systems profiling
#
# CONSISTENCY & REPRODUCIBILITY:
# - Fixed seed (42) for deterministic prompt generation
# - All inputs exactly 512 tokens (smart sampling: only prompts >= 512 tokens)
# - Deterministic generation (temperature=0.0, ignore_eos=false)
# - Consistent batching (4 requests = 4 batch size)
# - Comprehensive Nsight profiling with proper timing (60s delay, 5min profiling)
#
# PERFORMANCE TUNING VARIABLES:
# SCALE UP:  n_requests: 8, tensor_parallel_size: 8, max_num_batched_tokens: 4096
# SCALE DOWN: n_requests: 2, tensor_parallel_size: 2, max_num_batched_tokens: 1024
# INPUT SIZE: fixed_input_length: 256/512/1024/2048 (affects memory & compute)
# OUTPUT SIZE: max_tokens: 10/50/100 (affects generation time)
# PROFILING: nsight_duration: 120/300/600 (seconds), nsight_delay: 30/60/120 (model loading)
# LOAD: max_concurrency: 2/4/8, request_rate: 1.0/2.0/4.0 (requests per second)
# MEMORY: gpu_memory_utilization: 0.7/0.8/0.9 (GPU memory usage)

suite: "quantization_analysis_nsight"
name: "FP8 vs BF16 Performance Comparison with Nsight Profiling"
description: "Compare quantized vs full precision inference with detailed CUDA profiling"
tags: ["quantization", "memory", "efficiency", "nsight", "cuda", "profiling"]
n_requests: &n_requests 4

# Server Configuration - covers key vLLM parameters
server:
  # Model settings
  model: "meta-llama/Meta-Llama-3-8B"
  dtype: "bfloat16"
  trust_remote_code: false

  # Quantization settings
  quantization: "fp8"
  kv_cache_dtype: "auto"
  
  # Parallel processing
  tensor_parallel_size: 4
  pipeline_parallel_size: 1
  data_parallel_size: 1
  
  # Memory and caching
  gpu_memory_utilization: 0.9
  enable_prefix_caching: true
  block_size: 16
  swap_space: 4
  
  # Model length and batching
  max_model_len: 2048
  max_num_seqs: *n_requests
  max_num_batched_tokens: 2048  # Auto-calculated
  
  # Scheduling optimizations
  # enable_chunked_prefill: true
  # max_num_partial_prefills: 4
  # long_prefill_token_threshold: 1024
  #preemption_mode: "recompute"
  scheduling_policy: "fcfs"
  scheduler_delay_factor: 0.0     # Delay factor for batching
  
  # Performance features
  enforce_eager: false
  max_seq_len_to_capture: 8192
  disable_custom_all_reduce: false
  
  # Server connection
  host: "127.0.0.1"
  port: null  # Auto-assigned

# Benchmark Configuration
benchmark:
  # Backend and connection
  backend: "vllm"
  endpoint: "/v1/completions"
  
  # Dataset configuration
  dataset_name: "sharegpt"
  dataset_path: null  # Use default ShareGPT dataset
  num_prompts: *n_requests
  sharegpt_output_len: 5  # Override default output length
  
  # Traffic pattern - moderate load with some burstiness
  request_rate: "inf"  # Requests per second ("inf" = unlimited)
  burstiness: 1  # Traffic burstiness (1.0 = Poisson process)
  max_concurrency: *n_requests # Maximum concurrent requests
  
  # Sampling parameters (for consistent results)
  temperature: 0.0  # Greedy decoding
  top_p: 1.0
  top_k: -1
  min_p: 0.0
  max_tokens: 5
  
  # Request configuration
  use_beam_search: false
  logprobs: null
  ignore_eos: false
  
  # Input consistency options
  use_same_prompt: false  # Use different prompts for variety
  fixed_input_length: 512  # Set all inputs to exactly 512 tokens (truncate/pad as needed)
  prefer_truncation: true  # Prefer truncating long prompts over padding short ones
  
  # Metrics and analysis
  percentile_metrics: "ttft,tpot,itl,e2el"
  metric_percentiles: "50,90,95,99"
  goodput:
    - "ttft:200"    # 200ms TTFT SLO
    - "tpot:50"     # 50ms TPOT SLO
    - "e2el:5000"   # 5 second end-to-end SLO
  
  # Result management
  save_result: true
  save_detailed: true
  result_dir: "results"
  result_filename: null  # Auto-generated
  metadata:
    - "experiment_version=1.0"
    - "hardware_config=2xH100"
    - "optimization_level=production"
    - "profiling_method=nsight_systems"
  
  # Behavior
  seed: 42
  trust_remote_code: false
  disable_tqdm: false
  profile: true  # This will be automatically disabled when Nsight is enabled
  request_id_prefix: "profllm-quantization-nsight"

# System monitoring and profiling
system:
  # Resource monitoring
  monitor_gpu: true
  monitor_cpu: true
  monitor_memory: true
  monitoring_interval: 0.5  # High frequency monitoring
  
  # Disable torch profiler when using Nsight
  enable_profiling: false
  
  # Nsight Systems profiling configuration
  enable_nsight: true
  nsight_output_dir: "nsight_traces"  # Will be created as results/{run_id}/nsight_traces/
  nsight_trace_fork: true
  nsight_cuda_graph_trace: "node"
  nsight_delay: 60  # Wait 60 seconds before starting profiling (model loading time)
  nsight_duration: 300  # Profile for 5 minutes (None = manual stop)
  
  # High verbosity Nsight options for comprehensive analysis
  nsight_trace: "cuda,cudnn,mpi"  # Enable comprehensive tracing: CUDA, cuDNN, MPI, OS runtime
  nsight_cuda_trace_all_apis: true  # Trace all CUDA APIs (high overhead but comprehensive)
  nsight_cuda_memory_usage: true  # Track GPU memory usage by CUDA kernels
  nsight_cuda_backtrace: "all"  # Collect backtraces for all CUDA APIs
  nsight_cuda_flush_interval: 1000  # Flush CUDA buffers every 1 second
  nsight_sample: "process-tree"  # CPU sampling scope
  nsight_sampling_frequency: 8  # 8Hz sampling frequency (converted to period, results in 125000 period - minimum valid)
  nsight_cpu_core_events: "2"  # Instructions Retired (default)
  nsight_event_sample: "system-wide"  # Enable event sampling
  nsight_stats: true  # Generate summary statistics
  nsight_export: "text"  # Export additional text format
  
  # Additional tracing options (these are now included in nsight_trace above)
  nsight_mpi_trace: true  # Enable MPI tracing for collective operations
  nsight_osrt_trace: false  # Enable OS runtime tracing for system calls

# Export configuration
export:
  enable_export: true
  export_csv: true
  export_influxdb: false
  export_dir: "exports"
  csv_summary: true
  csv_requests: true
  csv_system: true
  
  # Performance optimizations
  use_streaming_export: false  # Enable for datasets > 10k requests
  csv_chunk_size: 1000  # Chunk size for streaming
  enable_fieldname_caching: true  # Cache fieldnames for performance
  
  use_timestamp_suffix: true
  timestamp_format: "%Y%m%d_%H%M%S"

# Output and execution
output_path: "results/quantization_comparison_nsight.json"
save_traces: true
timeout: 7200  # 2 hours
max_retries: 3
dry_run: false
